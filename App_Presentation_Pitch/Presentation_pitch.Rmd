---
title: "TEXT PREDICTION APP"
author: "Mohammed Teslim"
date: "2025-01-18"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

This presentation introduces a predictive text application that suggests the next word a user might type.

The app uses an n-gram language model trained on a large corpus of text from blogs, news articles, and Twitter posts.

Goal: To provide a fast and accurate word prediction experience.

# Algorithm

The app uses a statistical language model based on n-grams:

1.  **4-gram Model** with:
    -   Kneser-Ney smoothing
    -   Context window optimization
    -   Frequency-aware pruning (TOP_NGRAMS=500k)
2.  **Prediction Workflow:**
    -   Real-time input normalization
    -   Hash-based n-gram lookup
    -   Fallback cascade (4→3→2→1-gram)

# The App

The Shiny app provides a simple interface:

-   **Input:** A text box where the user enters a phrase.
-   **Output:**
    -   The top predicted next word.
    -   Showing the top 3 predictions.

**Instructions:**

1.  Type a phrase in the input box.
2.  Automatically generates the predicted next word(s).

**Link to App:** [Prediction_App](https://ml73o6-mohammed-teslim.shinyapps.io/Prediction_App//){.uri}

# How it Works

-   The app cleans the input text (removes punctuation, converts to lowercase, etc.).
-   It then uses the n-gram model to find the most probable next word based on the preceding words.
-   The backoff mechanism ensures that a prediction is always provided, even for unseen word combinations.
-   The app is optimized for speed by using efficient data structures (`data.table`) and pre-computed probabilities.

# Conclusion

This app demonstrates the power of n-gram models for next-word prediction.

**Future Improvements:**

-   Implement more advanced smoothing techniques.
-   Experiment with larger n-gram models (e.g., 5-grams).
-   Explore using recurrent neural networks (RNNs) for improved accuracy.

Thank you!
